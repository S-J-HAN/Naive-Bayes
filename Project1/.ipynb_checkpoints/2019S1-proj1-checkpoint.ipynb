{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Simon Jerome Han\n",
    "###### Python version: 3.7.1\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries that are useful\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(fileName, header=0):\n",
    "    \n",
    "    instances = []\n",
    "    labels = []\n",
    "    \n",
    "    labelsUnique = []\n",
    "    \n",
    "    # read data using IO, since pandas seems to be discouraged\n",
    "    with open(fileName, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            lineData = line.strip().split(\",\")\n",
    "            instance = lineData[:-1]\n",
    "            label = lineData[-1]\n",
    "            \n",
    "            instances.append(instance)\n",
    "            labels.append(label)\n",
    "            \n",
    "            if label not in labelsUnique:\n",
    "                labelsUnique.append(label)\n",
    "    \n",
    "    return (instances, labels), labelsUnique\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(data):\n",
    "\n",
    "    instances = data[0]\n",
    "    labels = data[1]\n",
    "\n",
    "    # Number of attributes\n",
    "    numberOfAttributes = len(instances[0])\n",
    "    numberOfInstances = len(instances)\n",
    "  \n",
    "    assert(numberOfInstances == len(labels))\n",
    "    \n",
    "    # unique list of labels\n",
    "    labelsUnique = []\n",
    "    labelCounts = Counter()\n",
    "    for label in labels:\n",
    "        labelCounts[label] += 1\n",
    "        if label not in labelsUnique:\n",
    "            labelsUnique.append(label)\n",
    "        \n",
    "    # Set up dictionary of posterior counts, and dictionary of posterior probabilities\n",
    "    posteriorCounts = {}\n",
    "    posteriorProbabilities = {}\n",
    "    for label in labelsUnique:\n",
    "        posteriorCounts[label] = []\n",
    "        posteriorProbabilities[label] = []\n",
    "        for i in range(0, numberOfAttributes):\n",
    "            posteriorCounts[label].append(Counter())\n",
    "            posteriorProbabilities[label].append({})\n",
    "    \n",
    "    # Get posterior counts from each instance in training data\n",
    "    for i in range(0, numberOfInstances):\n",
    "        instance = instances[i]\n",
    "        label = labels[i]\n",
    "        for j in range(0, numberOfAttributes):\n",
    "            attributeValue = instance[j]\n",
    "            if attributeValue not in ('?', ''):\n",
    "                posteriorCounts[label][j][attributeValue] += 1\n",
    "\n",
    "    # Get posterior probabilities\n",
    "    for label in labelsUnique:\n",
    "        attributeValueCounts = posteriorCounts[label]\n",
    "        for i in range(0, numberOfAttributes):\n",
    "            totalValueCount = sum(attributeValueCounts[i].values())\n",
    "            for attributeValue in attributeValueCounts[i].keys():\n",
    "                posteriorProbabilities[label][i][attributeValue] = attributeValueCounts[i][attributeValue] / totalValueCount\n",
    "   \n",
    "    # Get prior probabilities\n",
    "    priorProbabilities = {}\n",
    "    for label in labelsUnique:\n",
    "        priorProbabilities[label] = labelCounts[label] / numberOfInstances\n",
    "        \n",
    "    return (priorProbabilities, posteriorProbabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(model, instances, labelsUnique):\n",
    "    \n",
    "    priors = model[0]\n",
    "    posteriors = model[1]\n",
    "    predictions = []\n",
    "\n",
    "    for instance in instances:\n",
    "        # Keep track of label with the highest prediction value\n",
    "        maxProb = -np.inf\n",
    "        maxProbLabel = \"\"\n",
    "        \n",
    "        for label in labelsUnique:\n",
    "            if label in priors.keys():\n",
    "                logPrediction = np.log(priors[label])\n",
    "            else:\n",
    "                logPrediction = 0\n",
    "            for i in range(0, len(instance)):\n",
    "                if instance[i] not in ('?', '') and label in posteriors.keys():  \n",
    "                    if posteriors[label][i].get(instance[i], 0) != 0:\n",
    "                        logPrediction += np.log(posteriors[label][i].get(instance[i], 0))\n",
    "            \n",
    "            if logPrediction > maxProb:\n",
    "                maxProb = logPrediction\n",
    "                maxProbLabel = label  \n",
    "        \n",
    "        predictions.append(maxProbLabel)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(results, labelsUnique, micro = False, weighted = False):\n",
    "    \n",
    "    predictions = results[0]\n",
    "    actual = results[1]\n",
    "    \n",
    "    assert(len(predictions) == len(actual))\n",
    "    \n",
    "    # Precision and recall, by class label\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    \n",
    "    # True positive, false negative and false positive counts for each class label\n",
    "    tpLabel = {}\n",
    "    fpLabel = {}\n",
    "    fnLabel = {}\n",
    "    \n",
    "    # Get a simple measure of accuracy\n",
    "    correct = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if predictions[i] == actual[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct/len(predictions)\n",
    "    \n",
    "    # Get precision and recall for each label\n",
    "    for label in labelsUnique:\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for i in range(0, len(predictions)):\n",
    "            if predictions[i] == label:\n",
    "                if actual[i] == label:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if actual[i] == label:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "                    \n",
    "        tpLabel[label] = tp\n",
    "        fpLabel[label] = fp\n",
    "        fnLabel[label] = fn\n",
    "        \n",
    "        if not (tp == 0 and (fp == 0 or fn == 0)):\n",
    "            precision[label] = tp / (tp + fp)\n",
    "            recall[label] = tp / (tp + fn)\n",
    "        else:\n",
    "            precision[label] = 0\n",
    "            recall[label] = 0\n",
    "    \n",
    "    if not micro and not weighted:\n",
    "        # Macro Average, unweighted \n",
    "        precisionMacro = sum(precision.values())/len(labelsUnique)\n",
    "        recallMacro = sum(recall.values())/len(labelsUnique)\n",
    "        return accuracy, precisionMacro, recallMacro\n",
    "    \n",
    "    elif not micro and weighted:\n",
    "        # Weighted macro average\n",
    "        \n",
    "        # Get label counts\n",
    "        labelCounts = Counter()\n",
    "        for label in actual:\n",
    "            labelCounts[label] += 1\n",
    "        \n",
    "        precisionMacroWeighted = sum([(labelCounts[label]/len(actual))*precision[label] for label in labelsUnique])\n",
    "        recallMacroWeighted = sum([(labelCounts[label]/len(actual))*recall[label] for label in labelsUnique])\n",
    "        return accuracy, precisionMacroWeighted, recallMacroWeighted\n",
    "        \n",
    "    else:\n",
    "        # Micro Average\n",
    "        precisionMicro = sum(tpLabel.values()) / (sum(tpLabel.values()) + sum(fpLabel.values()))\n",
    "        recallMicro = sum(tpLabel.values()) / (sum(tpLabel.values()) + sum(fnLabel.values()))\n",
    "        return accuracy, precisionMicro, recallMicro\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(model, data, labelsUnique):\n",
    "    \n",
    "    priors = model[0]\n",
    "    posteriors = model[1]\n",
    "    instances = data[0]\n",
    "    labels = data[1]\n",
    "    \n",
    "    classEntropy = -sum([(priors[label]*np.log2(priors[label])) for label in labelsUnique])\n",
    "\n",
    "    numberOfAttributes = len(list(posteriors.values())[0])\n",
    "    attributeCounts = []\n",
    "    attributeLabelCounts = []\n",
    "    for i in range(0, numberOfAttributes):\n",
    "        attributeCounts.append(Counter())\n",
    "        attributeLabelCounts.append({})\n",
    "    \n",
    "    # Get attribute counts and label counts\n",
    "    for i in range(0, len(instances)):\n",
    "        for j in range(0, numberOfAttributes):\n",
    "            val = instances[i][j]\n",
    "            label = labels[i]\n",
    "            if val not in ('?', ''):\n",
    "                attributeCounts[j][val] += 1\n",
    "                attributeLabelCounts[j][val] = attributeLabelCounts[j].get(val, Counter())\n",
    "                attributeLabelCounts[j][val][label] += 1\n",
    "    \n",
    "    attributeEntropies = []\n",
    "    for i in range(0, numberOfAttributes):\n",
    "        counts = attributeLabelCounts[i]\n",
    "        attributeEntropies.append({})\n",
    "        for (key, val) in counts.items():\n",
    "            total = sum(val.values())\n",
    "            entropy = -sum([(v/total)*(np.log2(v/total)) for v in val.values()])\n",
    "            attributeEntropies[i][key] = entropy\n",
    "\n",
    "    informationGain = []\n",
    "    for i in range(0, numberOfAttributes):\n",
    "        counts = attributeCounts[i]\n",
    "        total = sum(counts.values())\n",
    "        meanInfo = sum([(counts[key]/total)*attributeEntropies[i][key] for key in counts.keys()])\n",
    "        gain = classEntropy - meanInfo\n",
    "        informationGain.append(gain)\n",
    "        \n",
    "    return informationGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processDataSet(name):\n",
    "    data, labelsUnique = preprocess(\"2019S1-proj1-data/{}.csv\".format(name), header=None)\n",
    "    model = train(data)\n",
    "    prediction = predict(model, data[0], labelsUnique)\n",
    "    results = (prediction, data[1])\n",
    "    accuracy, precision, recall = evaluate(results, labelsUnique, micro=False, weighted=True)\n",
    "    ig = info_gain(model, data, labelsUnique)\n",
    "    print(\"accuracy: {}\".format(accuracy))\n",
    "    print(\"precision: {}, recall: {}\".format(precision, recall))\n",
    "    print(\"information gain for each attribute: \\n{}\".format(ig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: anneal.csv\n",
      "accuracy: 0.22271714922049\n",
      "precision: 0.7738957788696961, recall: 0.22271714922048996\n",
      "information gain for each attribute: \n",
      "[0.40908953764450984, 0.0, 0.30605153542894037, 0.051344088764403883, 0.29108220585994704, 0.14711886228095583, 0.21372288031590858, 0.29223544065798435, 0.1261663361036094, 0.14107379163812883, 0.03248840649184159, 0.43517783626288553, 0.03870173274881039, 0.0004376065202116308, 0.03935557414283686, 0.021775078259213432, 0.03799747881351134, 0.03670308136440825, 0.0, 0.11722522630372034, 0.029753745208638716, 0.02704235332867655, 0.0, 0.015604780443500443, 0.13718113252042552, 0.0, 0.02239708985164568, 0.01824168402125026, 0.0, 0.0, 0.0, 0.043239605565149386, 0.033037571177056746, 0.019378864328319256, 0.003958783545891853]\n",
      "\n",
      "Dataset: breast-cancer.csv\n",
      "accuracy: 0.7377622377622378\n",
      "precision: 0.7351624344092063, recall: 0.7377622377622378\n",
      "information gain for each attribute: \n",
      "[0.010605956535614136, 0.0020016149737116518, 0.05717112532429669, 0.06899508808988608, 0.05716399333093369, 0.07700985251661452, 0.0024889884332655043, 0.012039330569516782, 0.025819023909141148]\n",
      "\n",
      "Dataset: car.csv\n",
      "accuracy: 0.05381944444444445\n",
      "precision: 0.7474886998229615, recall: 0.05381944444444444\n",
      "information gain for each attribute: \n",
      "[0.09644896916961399, 0.07370394692148596, 0.004485716626632108, 0.2196629633399082, 0.030008141247605424, 0.262184356554264]\n",
      "\n",
      "Dataset: cmc.csv\n",
      "accuracy: 0.4657162253903598\n",
      "precision: 0.48857300888894883, recall: 0.46571622539035984\n",
      "information gain for each attribute: \n",
      "[0.07090633894894571, 0.0401385992293839, 0.10173991727554088, 0.00982050143438462, 0.002582332379721386, 0.030474214560266333, 0.03251146005380656, 0.015786455595620197]\n",
      "\n",
      "Dataset: hepatitis.csv\n",
      "accuracy: 0.7935483870967742\n",
      "precision: 0.8312518496596627, recall: 0.7935483870967741\n",
      "information gain for each attribute: \n",
      "[0.03660746514280977, 0.010594083340565774, 0.014490701150154384, 0.08224158319201524, 0.07899847733968413, 0.00912525631068295, 0.04591830424559151, 0.04128454993985042, 0.03620910736150962, 0.10921215480315571, 0.13220153515592248, 0.07897096362576184, 0.08493296456638766]\n",
      "\n",
      "Dataset: hypothyroid.csv\n",
      "accuracy: 0.9317104015175466\n",
      "precision: 0.90584626512939, recall: 0.9317104015175466\n",
      "information gain for each attribute: \n",
      "[-0.0017990689210494737, 0.0009139351160850073, 0.0012382074503017315, 0.00014844815831743796, 0.0009985293906336068, 0.0013683791752741592, 0.0005423006444424394, 0.0004350938464638965, 0.0004888757691284829, 0.0008983004044028076, 4.463778824304043e-05, 7.8684698479492e-05, 0.009353710215580346, 0.004075493419623766, 0.005792553705846859, 0.005768288201614624, 0.005744031245602799, 0.002580427555574416]\n",
      "\n",
      "Dataset: mushroom.csv\n",
      "accuracy: 0.8471196454948301\n",
      "precision: 0.8503247705350682, recall: 0.8471196454948301\n",
      "information gain for each attribute: \n",
      "[0.048796701935373, 0.028590232773772706, 0.0360492829762038, 0.19237948576121955, 0.9060749773839998, 0.014165027250616191, 0.10088318399657026, 0.23015437514804604, 0.41697752341613126, 0.00751677256966421, 0.136965149496745, 0.2847255992184844, 0.2718944733927463, 0.2538451734622398, 0.24141556652756646, 0.0, 0.02381701612091669, 0.03845266924309043, 0.3180215107935376, 0.4807049176849153, 0.2019580190668523, 0.1568336046050921]\n",
      "\n",
      "Dataset: nursery.csv\n",
      "accuracy: 0.1003858024691358\n",
      "precision: 0.2002164431157993, recall: 0.1003858024691358\n",
      "information gain for each attribute: \n",
      "[0.07293460750309988, 0.1964492804881155, 0.005572591715219843, 0.011886431475775838, 0.019602025022872116, 0.0043331270252000564, 0.022232616894018342, 0.9587749604699762]\n",
      "\n",
      "Dataset: primary-tumor.csv\n",
      "accuracy: 0.11504424778761062\n",
      "precision: 0.13911561056616847, recall: 0.11504424778761062\n",
      "information gain for each attribute: \n",
      "[0.1547421418870596, 0.325571944982344, 0.3814674919993619, 0.7899338556821789, 0.2124618990481668, 0.020366938848049188, 0.10088123982399111, 0.06787277570442374, 0.22052193470670511, 0.1997614363902529, 0.06714460241010656, 0.04965187882304711, 0.29153013602249356, 0.12715354518198252, 0.2358358667150826, 0.18425767171538565, 0.17014811083887338]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataSetNames = [\"anneal\", \"breast-cancer\", \"car\", \"cmc\", \"hepatitis\", \"hypothyroid\", \"mushroom\", \"nursery\", \"primary-tumor\"]\n",
    "for name in dataSetNames:\n",
    "    print(\"Dataset: {}.csv\".format(name))\n",
    "    processDataSet(name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "Hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_TrainEvaluate_holdout(testPercentage, dataFileName):\n",
    "    \n",
    "    data, labelsUnique = preprocess(\"2019S1-proj1-data/{}.csv\".format(dataFileName), header=None)\n",
    "    attributes = data[0]\n",
    "    labels = data[1]\n",
    "    \n",
    "    attributeLabelPairs = [(attributes[i], labels[i]) for i in range(0, len(attributes))]\n",
    "    random.shuffle(attributeLabelPairs)\n",
    "    \n",
    "    numTest = int(testPercentage*len(attributes))\n",
    "    if numTest < 1:\n",
    "        numTest = 1\n",
    "    elif numTest == len(attributes):\n",
    "        numTest -= 1\n",
    "    testData = attributeLabelPairs[0:numTest]\n",
    "    trainData = attributeLabelPairs[numTest:]\n",
    "    \n",
    "    testAttributes = [x[0] for x in testData]\n",
    "    testLabels = [x[1] for x in testData]\n",
    "    trainAttributes = [x[0] for x in trainData]\n",
    "    trainLabels = [x[1] for x in trainData]\n",
    "    \n",
    "    model = train((trainAttributes, trainLabels))\n",
    "    prediction = predict(model, testAttributes, labelsUnique)\n",
    "    results = (prediction, testLabels)\n",
    "    accuracy, precision, recall = evaluate(results, labelsUnique, micro=False, weighted=True)\n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_RepeatedRandomSubsampling(testPercentage, iters, dataFileName):\n",
    "    cumulatedAccuracy = 0\n",
    "    cumulatedPrecision = 0\n",
    "    cumulatedRecall = 0\n",
    "    for i in range(iters):\n",
    "        accuracy, precision, recall = NB_TrainEvaluate_holdout(testPercentage, dataFileName)\n",
    "        cumulatedAccuracy += accuracy\n",
    "        cumulatedPrecision += precision\n",
    "        cumulatedRecall += recall\n",
    "    return cumulatedAccuracy/iters, cumulatedPrecision/iters, cumulatedRecall/iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_TrainEvaluate_CrossValidation(m, dataFileName):\n",
    "    data, labelsUnique = preprocess(\"2019S1-proj1-data/{}.csv\".format(dataFileName), header=None)\n",
    "    attributes = data[0]\n",
    "    labels = data[1]\n",
    "    \n",
    "    attributeLabelPairs = [(attributes[i], labels[i]) for i in range(0, len(attributes))]\n",
    "    random.shuffle(attributeLabelPairs)\n",
    "    \n",
    "    partitions = []\n",
    "    partitionSize = ceil(len(attributes)/m)\n",
    "    bottom = 0\n",
    "    top = partitionSize\n",
    "    for i in range(m):\n",
    "        partitions.append(attributeLabelPairs[bottom:top])\n",
    "        bottom += partitionSize\n",
    "        top += partitionSize\n",
    "        if top > len(attributes):\n",
    "            top = len(attributes)\n",
    "    \n",
    "    totalAccuracy = 0\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    for i in range(m):\n",
    "        testSet = partitions[i]\n",
    "        trainSet = [y for x in [partitions[j] for j in range(m) if j != i] for y in x]\n",
    "        \n",
    "        testAttributes = [x[0] for x in testSet]\n",
    "        testLabels = [x[1] for x in testSet]\n",
    "        trainAttributes = [x[0] for x in trainSet]\n",
    "        trainLabels = [x[1] for x in trainSet]\n",
    "        \n",
    "        model = train((trainAttributes, trainLabels))\n",
    "        prediction = predict(model, testAttributes, labelsUnique)\n",
    "        results = (prediction, testLabels)\n",
    "        accuracy, precision, recall = evaluate(results, labelsUnique, micro=False, weighted=True)\n",
    "        \n",
    "        totalAccuracy += accuracy\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "    \n",
    "    return totalAccuracy/m, totalPrecision/m, totalRecall/m\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_TrainEvaluate_Naive(dataFileName):\n",
    "    data, labelsUnique = preprocess(\"2019S1-proj1-data/{}.csv\".format(dataFileName), header=None)\n",
    "    model = train(data)\n",
    "    prediction = predict(model, data[0], labelsUnique)\n",
    "    results = (prediction, data[1])\n",
    "    return evaluate(results, labelsUnique, micro=False, weighted=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareEvaluationStrategies(dataSetName):\n",
    "    name = dataSetName\n",
    "    print(\"comparing evaluation strategies on the {}.csv dataset\".format(name))\n",
    "    print(\"                                    accuracy - precision - recall\")\n",
    "    print(\"Holdout (1 iteration):             {}\".format(NB_TrainEvaluate_holdout(0.2, name)))\n",
    "    print(\"Holdout with RRS ({} iterations): {}\".format(100, NB_RepeatedRandomSubsampling(0.2, 100, name)))\n",
    "    print(\"Cross Validation:                  {}\".format(NB_TrainEvaluate_CrossValidation(10, name)))\n",
    "    print(\"Train/Test on full dataset:        {}\".format(NB_TrainEvaluate_Naive(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing evaluation strategies on the anneal.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.16759776536312848, 0.7941706757086464, 0.16759776536312848)\n",
      "Holdout with RRS (100 iterations): (0.18737430167597768, 0.7720212195801538, 0.18737430167597768)\n",
      "Cross Validation:                  (0.20063131313131316, 0.7977885483345755, 0.2006313131313131)\n",
      "Train/Test on full dataset:        (0.22271714922049, 0.7738957788696961, 0.22271714922048996)\n",
      "\n",
      "comparing evaluation strategies on the breast-cancer.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.7192982456140351, 0.7192982456140351, 0.7192982456140351)\n",
      "Holdout with RRS (100 iterations): (0.7284210526315792, 0.7347288447702209, 0.7284210526315792)\n",
      "Cross Validation:                  (0.7311724137931034, 0.7656376250243527, 0.7311724137931034)\n",
      "Train/Test on full dataset:        (0.7377622377622378, 0.7351624344092063, 0.7377622377622378)\n",
      "\n",
      "comparing evaluation strategies on the car.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.06376811594202898, 0.734130076248297, 0.06376811594202898)\n",
      "Holdout with RRS (100 iterations): (0.048782608695652166, 0.4059645002120009, 0.048782608695652166)\n",
      "Cross Validation:                  (0.04801744245005577, 0.1916326073931031, 0.04801744245005577)\n",
      "Train/Test on full dataset:        (0.05381944444444445, 0.7474886998229615, 0.05381944444444444)\n",
      "\n",
      "comparing evaluation strategies on the cmc.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.4557823129251701, 0.4862219615364954, 0.45578231292517013)\n",
      "Holdout with RRS (100 iterations): (0.4526530612244897, 0.47630907804914685, 0.4526530612244897)\n",
      "Cross Validation:                  (0.4511357101782633, 0.47760869955889207, 0.4511357101782634)\n",
      "Train/Test on full dataset:        (0.4657162253903598, 0.48857300888894883, 0.46571622539035984)\n",
      "\n",
      "comparing evaluation strategies on the hepatitis.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.9032258064516129, 0.9081413210445469, 0.9032258064516129)\n",
      "Holdout with RRS (100 iterations): (0.7922580645161292, 0.8376307092321289, 0.7922580645161292)\n",
      "Cross Validation:                  (0.7863636363636364, 0.8380003850316351, 0.7863636363636364)\n",
      "Train/Test on full dataset:        (0.7935483870967742, 0.8312518496596627, 0.7935483870967741)\n",
      "\n",
      "comparing evaluation strategies on the hypothyroid.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.9335443037974683, 0.9005250518946639, 0.9335443037974682)\n",
      "Holdout with RRS (100 iterations): (0.9013132911392406, 0.9048595551951447, 0.9013132911392406)\n",
      "Cross Validation:                  (0.9172219395542891, 0.9079511776706746, 0.9172219395542891)\n",
      "Train/Test on full dataset:        (0.9317104015175466, 0.90584626512939, 0.9317104015175466)\n",
      "\n",
      "comparing evaluation strategies on the mushroom.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.8448275862068966, 0.8519079614604462, 0.8448275862068966)\n",
      "Holdout with RRS (100 iterations): (0.8471613300492612, 0.8503110659452295, 0.8471613300492613)\n",
      "Cross Validation:                  (0.8461579262632772, 0.8497140970176579, 0.8461579262632777)\n",
      "Train/Test on full dataset:        (0.8471196454948301, 0.8503247705350682, 0.8471196454948301)\n",
      "\n",
      "comparing evaluation strategies on the nursery.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.024691358024691357, 0.10489336774034384, 0.024691358024691357)\n",
      "Holdout with RRS (100 iterations): (0.028240740740740736, 0.09510449676127988, 0.028240740740740736)\n",
      "Cross Validation:                  (0.04614197530864197, 0.13426667423299793, 0.04614197530864197)\n",
      "Train/Test on full dataset:        (0.1003858024691358, 0.2002164431157993, 0.1003858024691358)\n",
      "\n",
      "comparing evaluation strategies on the primary-tumor.csv dataset\n",
      "                                    accuracy - precision - recall\n",
      "Holdout (1 iteration):             (0.07462686567164178, 0.04097161252560726, 0.07462686567164178)\n",
      "Holdout with RRS (100 iterations): (0.05328358208955224, 0.04150012679071744, 0.05328358208955224)\n",
      "Cross Validation:                  (0.09420677361853833, 0.08162727576851517, 0.09420677361853833)\n",
      "Train/Test on full dataset:        (0.11504424778761062, 0.13911561056616847, 0.11504424778761062)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in dataSetNames:\n",
    "    compareEvaluationStrategies(name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: Why isn't there much of a difference between the evaluation results?\n",
    "Hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithSpecialSmoothing(data, addK=False, k=0):\n",
    "\n",
    "    instances = data[0]\n",
    "    labels = data[1]\n",
    "\n",
    "    # Number of attributes\n",
    "    numberOfAttributes = len(instances[0])\n",
    "    numberOfInstances = len(instances)\n",
    "  \n",
    "    assert(numberOfInstances == len(labels))\n",
    "    \n",
    "    # unique list of labels\n",
    "    labelsUnique = []\n",
    "    labelCounts = Counter()\n",
    "    for label in labels:\n",
    "        labelCounts[label] += 1\n",
    "        if label not in labelsUnique:\n",
    "            labelsUnique.append(label)\n",
    "        \n",
    "    # Set up dictionary of posterior counts, and dictionary of posterior probabilities\n",
    "    posteriorCounts = {}\n",
    "    posteriorProbabilities = {}\n",
    "    for label in labelsUnique:\n",
    "        posteriorCounts[label] = []\n",
    "        posteriorProbabilities[label] = []\n",
    "        for i in range(0, numberOfAttributes):\n",
    "            posteriorCounts[label].append(Counter())\n",
    "            posteriorProbabilities[label].append({})\n",
    "            \n",
    "    # List of attribute values for each attribute\n",
    "    attributesUnique = []\n",
    "    for i in range(0, numberOfAttributes):\n",
    "        attributesUnique.append([])\n",
    "    \n",
    "    # Get posterior counts from each instance in training data\n",
    "    for i in range(0, numberOfInstances):\n",
    "        instance = instances[i]\n",
    "        label = labels[i]\n",
    "        for j in range(0, numberOfAttributes):\n",
    "            attributeValue = instance[j]\n",
    "            if attributeValue not in ('?', ''):\n",
    "                posteriorCounts[label][j][attributeValue] += 1\n",
    "                if attributeValue not in attributesUnique[j]:\n",
    "                    attributesUnique[j].append(attributeValue)\n",
    "\n",
    "    # Get posterior probabilities\n",
    "    for label in labelsUnique:\n",
    "        attributeValueCounts = posteriorCounts[label]\n",
    "        for i in range(0, numberOfAttributes):\n",
    "            totalValueCount = sum(attributeValueCounts[i].values())\n",
    "            for attributeValue in attributeValueCounts[i].keys():\n",
    "                if addK:\n",
    "                    posteriorProbabilities[label][i][attributeValue] = (k + attributeValueCounts[i][attributeValue])/(len(attributesUnique[i]) + labelCounts[label])\n",
    "                else:\n",
    "                    posteriorProbabilities[label][i][attributeValue] = attributeValueCounts[i][attributeValue] / totalValueCount\n",
    "   \n",
    "    # Get prior probabilities\n",
    "    priorProbabilities = {}\n",
    "    for label in labelsUnique:\n",
    "        priorProbabilities[label] = labelCounts[label] / numberOfInstances\n",
    "        \n",
    "    return (priorProbabilities, posteriorProbabilities), (labelCounts, [len(x) for x in attributesUnique])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWithSpecialSmoothing(model, instances, labelsUnique, counts, addK=False, k=0):\n",
    "    \n",
    "    priors = model[0]\n",
    "    posteriors = model[1]\n",
    "    predictions = []\n",
    "    \n",
    "    labelCounts = counts[0]\n",
    "    attributeCounts = counts[1]\n",
    "\n",
    "    for instance in instances:\n",
    "        # Keep track of label with the highest prediction value\n",
    "        maxProb = -np.inf\n",
    "        maxProbLabel = \"\"\n",
    "        \n",
    "        for label in labelsUnique:\n",
    "            if label in priors.keys():\n",
    "                logPrediction = np.log(priors[label])\n",
    "            else:\n",
    "                logPrediction = 0\n",
    "            for i in range(0, len(instance)):\n",
    "                if instance[i] not in ('?', '') and label in posteriors.keys(): \n",
    "                    if posteriors[label][i].get(instance[i], 0) != 0:\n",
    "                        logPrediction += np.log(posteriors[label][i].get(instance[i], 0))\n",
    "                    elif addK:\n",
    "                        logPrediction += np.log(k/(attributeCounts[i] + labelCounts[label]))\n",
    "            \n",
    "            if logPrediction > maxProb:\n",
    "                maxProb = logPrediction\n",
    "                maxProbLabel = label  \n",
    "        \n",
    "        predictions.append(maxProbLabel)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_TrainEvaluate_SpecialSmoothing_CrossValidation(dataFileName, m=10, addK=False, k=0):\n",
    "    data, labelsUnique = preprocess(\"2019S1-proj1-data/{}.csv\".format(dataFileName), header=None)\n",
    "    attributes = data[0]\n",
    "    labels = data[1]\n",
    "\n",
    "    attributeLabelPairs = [(attributes[i], labels[i]) for i in range(0, len(attributes))]\n",
    "    random.shuffle(attributeLabelPairs)\n",
    "\n",
    "    partitions = []\n",
    "    partitionSize = ceil(len(attributes)/m)\n",
    "    bottom = 0\n",
    "    top = partitionSize\n",
    "    for i in range(m):\n",
    "        partitions.append(attributeLabelPairs[bottom:top])\n",
    "        bottom += partitionSize\n",
    "        top += partitionSize\n",
    "        if top > len(attributes):\n",
    "            top = len(attributes)\n",
    "\n",
    "    totalAccuracy = 0\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    for i in range(m):\n",
    "        testSet = partitions[i]\n",
    "        trainSet = [y for x in [partitions[j] for j in range(m) if j != i] for y in x]\n",
    "\n",
    "        testAttributes = [x[0] for x in testSet]\n",
    "        testLabels = [x[1] for x in testSet]\n",
    "        trainAttributes = [x[0] for x in trainSet]\n",
    "        trainLabels = [x[1] for x in trainSet]\n",
    "  \n",
    "        model, counts = trainWithSpecialSmoothing((trainAttributes, trainLabels), addK=addK, k=k)\n",
    "        prediction = predictWithSpecialSmoothing(model, testAttributes, labelsUnique, counts, addK=addK, k=k)\n",
    "        results = (prediction, testLabels)\n",
    "        accuracy, precision, recall = evaluate(results, labelsUnique, micro=False, weighted=True)\n",
    "\n",
    "        totalAccuracy += accuracy\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "\n",
    "    return totalAccuracy/m, totalPrecision/m, totalRecall/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: anneal.csv\n",
      "no smoothing:          Accuracy: 0.20287878787878788   Precision: 0.772739264965789   Recall: 0.20287878787878783\n",
      "using addK smoothing:  Accuracy: 0.9198484848484849   Precision: 0.9579714303454976   Recall: 0.9198484848484849\n",
      "\n",
      "dataset: breast-cancer.csv\n",
      "no smoothing:          Accuracy: 0.7190344827586206   Precision: 0.7280325447166527   Recall: 0.7190344827586206\n",
      "using addK smoothing:  Accuracy: 0.7295172413793103   Precision: 0.7381726184526785   Recall: 0.7295172413793103\n",
      "\n",
      "dataset: car.csv\n",
      "no smoothing:          Accuracy: 0.04515431159787715   Precision: 0.33169744252339195   Recall: 0.04515431159787715\n",
      "using addK smoothing:  Accuracy: 0.8506980360342087   Precision: 0.849522983570585   Recall: 0.8506980360342089\n",
      "\n",
      "dataset: cmc.csv\n",
      "no smoothing:          Accuracy: 0.4530237684493004   Precision: 0.478056902869768   Recall: 0.4530237684493004\n",
      "using addK smoothing:  Accuracy: 0.49856718420548213   Precision: 0.5152636881163952   Recall: 0.49856718420548213\n",
      "\n",
      "dataset: hepatitis.csv\n",
      "no smoothing:          Accuracy: 0.7977272727272727   Precision: 0.8446144480519482   Recall: 0.7977272727272727\n",
      "using addK smoothing:  Accuracy: 0.8289772727272726   Precision: 0.901113782051282   Recall: 0.8289772727272726\n",
      "\n",
      "dataset: hypothyroid.csv\n",
      "no smoothing:          Accuracy: 0.9209361961941589   Precision: 0.9064368028122309   Recall: 0.9209361961941589\n",
      "using addK smoothing:  Accuracy: 0.9516352905261016   Precision: 0.9069418183475353   Recall: 0.9516352905261016\n",
      "\n",
      "dataset: mushroom.csv\n",
      "no smoothing:          Accuracy: 0.8466325555448864   Precision: 0.8503116834841759   Recall: 0.8466325555448861\n",
      "using addK smoothing:  Accuracy: 0.9551863994476374   Precision: 0.9580541096823515   Recall: 0.9551863994476376\n",
      "\n",
      "dataset: nursery.csv\n",
      "no smoothing:          Accuracy: 0.04753086419753086   Precision: 0.13475116231403353   Recall: 0.04753086419753087\n",
      "using addK smoothing:  Accuracy: 0.9021604938271605   Precision: 0.898651000598569   Recall: 0.9021604938271605\n",
      "\n",
      "dataset: primary-tumor.csv\n",
      "no smoothing:          Accuracy: 0.08279857397504456   Precision: 0.07162035788765893   Recall: 0.08279857397504456\n",
      "using addK smoothing:  Accuracy: 0.36185383244206776   Precision: 0.33830997887855396   Recall: 0.36185383244206776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in dataSetNames:\n",
    "    addK_accuracy, addK_precision, addK_recall = NB_TrainEvaluate_SpecialSmoothing_CrossValidation(name, m=10, addK=True, k=1)\n",
    "    accuracy, precision, recall = NB_TrainEvaluate_SpecialSmoothing_CrossValidation(name, m=10, addK=False)\n",
    "    print(\"dataset: {}.csv\".format(name))\n",
    "    print(\"no smoothing:          Accuracy: {}   Precision: {}   Recall: {}\".format(accuracy, precision, recall))\n",
    "    print(\"using addK smoothing:  Accuracy: {}   Precision: {}   Recall: {}\".format(addK_accuracy, addK_precision, addK_recall))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
